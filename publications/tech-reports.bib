@article{amos2017optnet,
  title = "{OptNet: Differentiable Optimization as a Layer in Neural Networks}",
  author={Brandon Amos and J. Zico Kolter},
  journal={arXiv preprint arXiv:1703.00443},
  _venue={arXiv},
  codeurl={https://github.com/locuslab/optnet},
  year={2017},
  url={http://arxiv.org/abs/1703.00443},
  abstract={
    This paper presents OptNet, a network architecture that integrates
    optimization problems (here, specifically in the form of quadratic
    programs) as individual layers in larger end-to-end trainable deep
    networks. These layers allow complex dependencies between the hidden
    states to be captured that traditional convolutional and
    fully-connected layers are not able to capture. In this paper, we
    develop the foundations for such an architecture: we derive the
    equations to perform exact differentiation through these layers and
    with respect to layer parameters; we develop a highly efficient solver
    for these layers that exploits fast GPU-based batch solves within a
    primal-dual interior point method, and which provides backpropagation
    gradients with virtually no additional cost on top of the solve; and
    we highlight the application of these approaches in several
    problems. In one particularly standout example, we show that the
    method is capable of learning to play Sudoku given just input and
    output games, with no a priori information about the rules of the
    game; this task is virtually impossible for other neural network
    architectures that we have experimented with, and highlights the
    representation capabilities of our approach.
  }
}

@article{donti2017task,
  title={Task-based End-to-end Model Learning},
  author={Donti, Priya L and Amos, Brandon and Kolter, J Zico},
  journal={arXiv preprint arXiv:1703.04529},
  year={2017},
  _venue={arXiv},
  codeurl={https://github.com/locuslab/e2e-model-learning},
  url={http://arxiv.org/abs/1703.04529},
  abstract={
    As machine learning techniques have become more ubiquitous, it has
    become common to see machine learning prediction algorithms operating
    within some larger process. However, the criteria by which we train
    machine learning algorithms often differ from the ultimate criteria on
    which we evaluate them. This paper proposes an end-to-end approach for
    learning probabilistic machine learning models within the context of
    stochastic programming, in a manner that directly captures the
    ultimate task-based objective for which they will be used. We then
    present two experimental evaluations of the proposed approach, one as
    applied to a generic inventory stock problem and the second to a
    real-world electrical grid scheduling task. In both cases, we show
    that the proposed approach can outperform both a traditional modeling
    approach and a purely black-box policy optimization approach.
  }
}

@article{amos2016input,
  title={Input Convex Neural Networks},
  author={Brandon Amos and Lei Xu and J. Zico Kolter},
  journal={arXiv preprint arXiv:1609.07152},
  _venue={arXiv},
  codeurl={https://github.com/locuslab/icnn},
  year={2016},
  url={http://arxiv.org/abs/1609.07152},
  abstract={
    This paper presents the input convex neural network
    architecture. These are scalar-valued (potentially deep) neural
    networks with constraints on the network parameters such that the
    output of the network is a convex function of (some of) the
    inputs. The networks allow for efficient inference via optimization
    over some inputs to the network given others, and can be applied to
    settings including structured prediction, data imputation,
    reinforcement learning, and others. In this paper we lay the basic
    groundwork for these models, proposing methods for inference,
    optimization and learning, and analyze their representational
    power. We show that many existing neural network architectures can be
    made input-convex with only minor modification, and develop
    specialized optimization algorithms tailored to this setting. Finally,
    we highlight the performance of the methods on multi-label prediction,
    image completion, and reinforcement learning problems, where we show
    improvement over the existing state of the art in many cases.
  }
}

@techreport{amos2016openface,
  title={OpenFace: A general-purpose face recognition
    library with mobile applications},
  author={Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev},
  _venue={CMU},
  year={2016},
  institution={Technical Report CMU-CS-16-118, CMU School of Computer Science},
  url={http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf},
  codeurl={https://cmusatyalab.github.io/openface},
  abstract={
    Cameras are becoming ubiquitous in the Internet of Things (IoT) and
    can use face recognition technology to improve context. There is a
    large accuracy gap between today's publicly available face recognition
    systems and the state-of-the-art private face recognition
    systems. This paper presents our OpenFace face recognition library
    that bridges this accuracy gap. We show that OpenFace provides
    near-human accuracy on the LFW benchmark and present a new
    classification benchmark for mobile scenarios. This paper is intended
    for non-experts interested in using OpenFace and provides a light
    introduction to the deep neural network techniques we use.

    We released OpenFace in October 2015 as an open source library under
    the Apache 2.0 license. It is available at:
    <http://cmusatyalab.github.io/openface/>
  }
}

@techreport{gao2015cloudlets,
  title={Are Cloudlets Necessary?},
  author={
    Gao, Ying and Hu, Wenlu and Ha, Kiryong and
    Amos, Brandon and Pillai, Padmanabhan and
    Satyanarayanan, Mahadev
  },
  _venue={CMU},
  year={2015},
  institution={Technical Report CMU-CS-15-139, CMU School of Computer Science},
  url={http://reports-archive.adm.cs.cmu.edu/anon/anon/2015/CMU-CS-15-139.pdf},
  abstract={
    We present experimental results from Wi-Fi and 4G LTE networks to validate the
    intuition that low end-to-end latency of cloud services improves application
    response time and reduces energy consumption on mobile devices. We focus
    specifically on computational offloading as a cloud service. Using a wide
    range of applications, and exploring both pre-partitioned and dynamically
    partitioned approaches, we demonstrate the importance of low latency for
    cloud offload services. We show the best performance is achieved by
    offloading to cloudlets, which are small-scale edge-located data centers. Our
    results show that cloudlets can improve response times 51\% and reduce energy
    consumption in a mobile device by up to 42\% compared to cloud offload.
  }
}

@techreport{ha2015adaptive,
  title={Adaptive VM handoff across cloudlets},
  author={
    Ha, Kiryong and Abe, Yoshihisa and Chen, Zhuo and
    Hu, Wenlu and Amos, Brandon and Pillai, Padmanabhan and
    Satyanarayanan, Mahadev
  },
  _venue={CMU},
  year={2015},
  institution={Technical Report CMU-CS-15-113, CMU School of Computer Science},
  url={http://ra.adm.cs.cmu.edu/anon/2015/CMU-CS-15-113.pdf},
  abstract={
    Cloudlet offload is a valuable technique for ensuring low end-to-end latency of
    resource-intensive cloud processing for many emerging mobile applications.
    This paper examines the impact of user mobility on cloudlet offload, and
    shows that even modest user mobility can result in significant network
    degradation. We propose VM handoff as a technique for seamlessly transferring
    VMencapsulated execution to a more optimal offload site as users move. Our
    approach can perform handoff in roughly a minute even over limited WANs by
    adaptively reducing data transferred. We present experimental results to
    validate our implementation and to demonstrate effectiveness of adaptation to
    changing network conditions and processing capacity
  }
}

@article{amos2014QNSTOP,
  title={{{QNSTOP-QuasiNewton Algorithm for Stochastic Optimization}}},
  author={Brandon Amos and David Easterling and Layne Watson and
    William Thacker and Brent Castle and Michael Trosset},
  journal={},
  _venue={VT},
  year={2014},
  keywords={journal},
  url={https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qnTOMS14.pdf},
  abstract={
    QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the
    quasi-Newton stochastic optimization method of Castle and Trosset. For
    stochastic problems, convergence theory exists for the particular
    algorithmic choices and parameter values used in QNSTOP. Both the parallel
    driver subroutine, which offers several parallel decomposition strategies,
    and the serial driver subroutine can be used for stochastic optimization or
    deterministic global optimization, based on an input switch. QNSTOP is
    particularly effective for “noisy” deterministic problems, using only
    objective function values. Some performance data for computational systems
    biology problems is given.
  }
}
